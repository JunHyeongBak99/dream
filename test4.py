import os
import streamlit as st
import openai
import requests
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI

# OpenAI API í‚¤ ì„¤ì •
openai_api_key = "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"
openai.api_key = openai_api_key

# ë„¤ì´ë²„ API ì„¤ì •
CLIENT_ID = "uh2AfBPLQDIUkXpkvnQC"
CLIENT_SECRET = "dXBin21ECU"

# ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def search_dream_with_naver(query):
    url = f"https://openapi.naver.com/v1/search/encyc.json?query={query}"
    headers = {
        "X-Naver-Client-Id": CLIENT_ID,
        "X-Naver-Client-Secret": CLIENT_SECRET,
    }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        items = response.json().get("items", [])
        return [f"{item['title']} - {item['description']}" for item in items]
    except requests.exceptions.RequestException as e:
        st.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
        return []

# LangChain RAG êµ¬ì„± í•¨ìˆ˜
def initialize_rag_system(naver_results):
    if not naver_results:
        st.warning("ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        naver_results = ["ê¸°ë³¸ ë°ì´í„° ì˜ˆì‹œ"]

    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vectorstore = Chroma.from_texts(naver_results, embeddings)

    llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    return ConversationalRetrievalChain.from_llm(
        llm=llm, retriever=retriever, memory=memory
    )

# Streamlit UI êµ¬ì„±
def run_ui():
    st.markdown(
        """
        <style>
        .stApp {
            background: linear-gradient(to bottom, #0b1e2e, #f8c9d4);
            color: #ffffff;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    st.title("ğŸŒ™ ë‹¹ì‹ ì˜ ë°¤ì€ ì•ˆë…•í•˜ì‹ ê°€ìš”?")
    st.markdown("ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ì™€ GPTë¥¼ í™œìš©í•œ í†µí•© í•´ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.")

    dream_description = st.text_area("ê¿ˆ ì„¤ëª…ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ìˆ² ì†ì—ì„œ ê¸¸ì„ ìƒê³  ì–´ë‘  ì†ì—ì„œ ì«“ê¸°ëŠ” ê¿ˆì„ ê¾¸ì—ˆìŠµë‹ˆë‹¤.")
    stress_score = st.slider("ìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€ì„ ì„ íƒí•˜ì„¸ìš”", 0, 100, 50)

    if st.button("í•´ì„ ìš”ì²­"):
        if dream_description:
            st.subheader("ğŸ’¬ GPT í•´ëª½ ë° ì¡°ì–¸")
            question = f"ê¿ˆ ì„¤ëª…: {dream_description}\nìŠ¤íŠ¸ë ˆìŠ¤ ìˆ˜ì¤€: {stress_score}/100\n"
            llm = OpenAI(temperature=0.7, openai_api_key=openai_api_key)

            try:
                #question ë³€ìˆ˜ê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                st.write(f"ì „ë‹¬ëœ ì§ˆë¬¸: {question}")
                st.write(f"ì§ˆë¬¸ íƒ€ì…: {type(question)}")
                response = llm.generate([question])
                st.write(response.generations[0][0].text.strip())
            except openai.OpenAIError as e:
                st.error(f"OpenAI API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            except ValueError as ve:
                st.error(f"ì…ë ¥ ê°’ ì˜¤ë¥˜: {ve}")
            except Exception as e:
                st.error(f"ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

            st.subheader("ğŸ” ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼")
            naver_results = search_dream_with_naver(dream_description + " ê¿ˆ í•´ëª½")
            if naver_results:
                for result in naver_results:
                    st.write(f"- {result}")
            else:
                st.write("ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("ê¿ˆ ì„¤ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")

# ì‹¤í–‰
if __name__ == "__main__":
    run_ui()
